{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_baseline1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Model.py"
      ],
      "metadata": {
        "id": "bKLAGatDVGE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install positional-encodings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMNp2LT6W1-U",
        "outputId": "eb750e54-318d-4273-80a0-4559bf142ab1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting positional-encodings\n",
            "  Downloading positional_encodings-5.0.0-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from positional-encodings) (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from positional-encodings) (1.21.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from positional-encodings) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (0.24.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (1.13.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (3.17.3)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (1.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (1.43.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 28.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (3.10.0.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (2.8.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (13.0.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->positional-encodings) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->positional-encodings) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->positional-encodings) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (4.11.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->positional-encodings) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly, positional-encodings\n",
            "Successfully installed positional-encodings-5.0.0 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "9GzktvslE6oe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from positional_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,in_channels = 1):\n",
        "    super(Encoder,self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels = in_channels,out_channels = 64,kernel_size=(3,3),padding=(1,1),stride=(1,1))\n",
        "    self.conv2 = nn.Conv2d(in_channels = 64,out_channels = 128 ,kernel_size=(3,3),padding=(1,1),stride=(1,1))\n",
        "    self.conv3 = nn.Conv2d(in_channels = 128,out_channels = 256 ,kernel_size=(3,3),padding=(1,1),stride=(1,1))\n",
        "    self.conv3_ = nn.Conv2d(in_channels = 256,out_channels = 256 ,kernel_size=(3,3),padding=(1,1),stride=(1,1))\n",
        "    self.conv4 = nn.Conv2d(in_channels = 256,out_channels = 512 ,kernel_size=(3,3),padding=(1,1),stride=(1,1))\n",
        "    self.max_pool = nn.MaxPool2d(kernel_size=(2,2),padding=(0,0),stride=(2,2))\n",
        "    self.max_pool2 = nn.MaxPool2d(kernel_size=(2,1),padding=(0,0),stride=(2,1))\n",
        "    self.batch_norm2 = nn.BatchNorm2d(num_features=512)\n",
        "    self.batch_norm1 = nn.BatchNorm2d(num_features=256)\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x = self.conv1(x)\n",
        "    x = self.max_pool(x)\n",
        "    x = self.conv2(x)\n",
        "    x = self.max_pool(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.batch_norm1(x) \n",
        "    x = self.conv3_(x)\n",
        "    x = self.max_pool(x)\n",
        "    x = self.conv4(x)\n",
        "    x = self.batch_norm2(x)\n",
        "    p_enc_2d = PositionalEncodingPermute2D(x.shape[1])\n",
        "    penc_x = p_enc_2d(x)\n",
        "    assert penc_x.size()==x.size()\n",
        "    combined_x = penc_x+x\n",
        "    # print(combined_x.shape)\n",
        "#### NOT SURE IF THEY WANT THIS SHAPE\n",
        "    # x = x.reshape(x.shape[0],-1)\n",
        "#### OR (MOSTLY THIS)\n",
        "    x = x.reshape(x.shape[0],x.shape[2]*x.shape[3],1,x.shape[1])\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the encoder\n",
        "encoder = Encoder(in_channels=1)\n",
        "x = torch.randn(64,1,28,28)\n",
        "# print(x.shape)\n",
        "x=encoder(x)\n",
        "print(x.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYBxUj1qa17o",
        "outputId": "e3e0b00d-0c30-49df-87cd-df22c5555daa"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 9, 1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as utils\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn.parameter import Parameter\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, cell_size, hidden_size):\n",
        "        \"\"\"\n",
        "        cell_size is the size of cell_state.\n",
        "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
        "        \"\"\"\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        self.cell_size = cell_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fl = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.il = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.ol = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.Cl = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        \n",
        "    def step(self, input, Hidden_State, Cell_State):\n",
        "        combined = torch.cat((input, Hidden_State), 1)\n",
        "        f = F.sigmoid(self.fl(combined))\n",
        "        i = F.sigmoid(self.il(combined))\n",
        "        o = F.sigmoid(self.ol(combined))\n",
        "        C = F.tanh(self.Cl(combined))\n",
        "        Cell_State = f * Cell_State + i * C\n",
        "        Hidden_State = o * F.tanh(Cell_State)\n",
        "        \n",
        "        return Hidden_State, Cell_State\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        batch_size = inputs.size(0)\n",
        "        time_step = inputs.size(1)\n",
        "        Hidden_State, Cell_State = self.initHidden(batch_size)\n",
        "        outputs = None\n",
        "        for i in range(time_step):\n",
        "            Hidden_State, Cell_State = self.step(torch.squeeze(inputs[:,i:i+1,:]), Hidden_State, Cell_State)  \n",
        "            if outputs is None:\n",
        "                outputs = Hidden_State.unsqueeze(1)\n",
        "            else:\n",
        "                outputs = torch.cat((Hidden_State.unsqueeze(1), outputs), 1)\n",
        "        return outputs\n",
        "    \n",
        "    def initHidden(self, batch_size):\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        if use_gpu:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            return Hidden_State, Cell_State\n",
        "        else:\n",
        "            Hidden_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            Cell_State = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            return Hidden_State, Cell_State\n",
        "        \n",
        "class BiLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_size, cell_size, hidden_size):\n",
        "        \"\"\"\n",
        "        cell_size is the size of cell_state.\n",
        "        hidden_size is the size of hidden_state, or say the output_state of each step\n",
        "        \"\"\"\n",
        "        super(BiLSTM, self).__init__()\n",
        "        \n",
        "        self.cell_size = cell_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.fl_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.il_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.ol_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.Cl_f = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.fl_b = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.il_b = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.ol_b = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.Cl_b = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        \n",
        "        \n",
        "    \n",
        "    def step(self, input_f, input_b, Hidden_State_f, Cell_State_f, Hidden_State_b, Cell_State_b):\n",
        "        batch_size = input_f.size(0)\n",
        "        \n",
        "        combined_f = torch.cat((input_f, Hidden_State_f), 1)\n",
        "        \n",
        "        f_f = F.sigmoid(self.fl_f(combined_f))\n",
        "        i_f = F.sigmoid(self.il_f(combined_f))\n",
        "        o_f = F.sigmoid(self.ol_f(combined_f))\n",
        "        C_f = F.tanh(self.Cl_f(combined_f))\n",
        "        Cell_State_f = f_f * Cell_State_f + i_f * C_f\n",
        "        Hidden_State_f = o_f * F.tanh(Cell_State_f)\n",
        "        \n",
        "        combined_b = torch.cat((input_b, Hidden_State_b), 1)\n",
        "\n",
        "        f_b = F.sigmoid(self.fl_b(combined_b))\n",
        "        i_b = F.sigmoid(self.il_b(combined_b))\n",
        "        o_b = F.sigmoid(self.ol_b(combined_b))\n",
        "        C_b = F.tanh(self.Cl_b(combined_b))\n",
        "        Cell_State_b = f_b * Cell_State_b + i_b * C_b\n",
        "        Hidden_State_b = o_b * F.tanh(Cell_State_b)\n",
        "        \n",
        "        return Hidden_State_f, Cell_State_f, Hidden_State_b, Cell_State_b\n",
        "    \n",
        "    def forward(self, inputs):  \n",
        "        outputs_f = None\n",
        "        outputs_b = None\n",
        "        \n",
        "        batch_size = inputs.size(0)\n",
        "        steps = inputs.size(1)\n",
        "        \n",
        "        Hidden_State_f, Cell_State_f, Hidden_State_b, Cell_State_b = self.initHidden(batch_size)\n",
        "        \n",
        "        for i in range(steps):\n",
        "            Hidden_State_f, Cell_State_f, Hidden_State_b, Cell_State_b = \\\n",
        "                self.step(torch.squeeze(inputs[:,i:i+1,:]), torch.squeeze(inputs[:,steps-i-1:steps-i,:])\\\n",
        "                          , Hidden_State_f, Cell_State_f, Hidden_State_b, Cell_State_b)  \n",
        "            \n",
        "            if outputs_f is None:\n",
        "                outputs_f = Hidden_State_f.unsqueeze(1)\n",
        "            else:\n",
        "                outputs_f = torch.cat((outputs_f, Hidden_State_f.unsqueeze(1)), 1)\n",
        "            if outputs_b is None:\n",
        "                outputs_b = Hidden_State_b.unsqueeze(1)\n",
        "            else:\n",
        "                outputs_b = torch.cat((Hidden_State_b.unsqueeze(1), outputs_b), 1)\n",
        "        outputs = (outputs_f + outputs_b) / 2\n",
        "        return outputs\n",
        "        \n",
        "    \n",
        "    def initHidden(self, batch_size):\n",
        "        use_gpu = torch.cuda.is_available()\n",
        "        if use_gpu:\n",
        "            Hidden_State_f = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            Cell_State_f = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            Hidden_State_b = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            Cell_State_b = Variable(torch.zeros(batch_size, self.hidden_size).cuda())\n",
        "            return Hidden_State_f, Cell_State_f, Hidden_State_b, Cell_State_b\n",
        "        else:\n",
        "            Hidden_State_f = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            Cell_State_f = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            Hidden_State_b = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            Cell_State_b = Variable(torch.zeros(batch_size, self.hidden_size))\n",
        "            return Hidden_State_f, Cell_State_f, Hidden_State_b, Cell_State_b"
      ],
      "metadata": {
        "id": "5ZGCP1uYUk7T"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}