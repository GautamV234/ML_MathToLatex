import pandas as pd
import os
import re
import codecs
from six.moves import cPickle as pickle
import string
from PIL import Image

pd.options.display.max_rows = 999
pd.options.display.max_columns = 20
pd.options.display.max_colwidth = 100
pd.options.display.width = 160

data_dir = data_folder = 'C:\work\ml_PDF_parsing\ML_MathToLatex\dataset'
image_folder = image_dir = os.path.join(data_dir,'formula_images')
output_dir = 'step2'
dump = True # set this to True to dump data to disk, otherwise set it to False.


def makeDatasetDetails(data_dir):
    pickle_path = 'C:\work\ml_PDF_parsing\ML_MathToLatex\dataset\df1_dataset_details.pkl'
    if (not dump) and os.path.exists(pickle_path):
        raise Exception('File %s already exists'%pickle_path)
    widths=[]
    heights=[]
    formula_lens=[]
    # step1/im2latex_dataset_map.df.pkl is generated by formula_list.py in the repo.
    # untrix/im2latex-dataset
    image_path_g = 'C:\work\ml_PDF_parsing\ML_MathToLatex\dataset\images'
    # datasetDF = pd.read_pickle(os.path.join(data_dir, 'step1', 'im2latex_dataset_map.df.pkl'))
    data_g =  pd.read_csv('C:\work\ml_PDF_parsing\ML_MathToLatex\dataset\data.csv')
    for index,row in data_g.iterrows():
        image_name = row['image']
        formula = row['latex']
        formula_lens.append(len(formula))
        im = Image.open(os.path.join(image_path_g,image_name))
        widths.append(im.size[0])
        heights.append(im.size[1])

    # for _, row in datasetDF.iterrows():
    #     image_name = row.image
    #     im = Image.open(os.path.join(image_folder,image_name))
    #     widths.append(im.size[0])
    #     heights.append(im.size[1])
    #     formula_lens.append(len(row.latex))
    print(len(widths), len(heights))
    data_g = data_g.assign(width=widths, height=heights, formula_len=formula_lens)
    if not os.path.exists(os.path.join(data_dir, output_dir)):
        os.makedirs(os.path.join(data_dir, output_dir))
    if dump:
        data_g.to_pickle(pickle_path)
    return data_g
    
def getDatasetDetails(data_dir):
    try:
        # df = pd.read_pickle(os.path.join(data_dir, output_dir, 'df1_dataset_details.pkl'))
        pickle_path = os.path.join(data_dir,'df1_dataset_details.pkl')
        # pickle_path = 'C:\work\ml_PDF_parsing\ML_MathToLatex\dataset\df1_dataset_details.pkl'
        df = pd.read_pickle(pickle_path)
    except:
        df = makeDatasetDetails(data_dir)
    print (df.shape)
    # display(df.iloc[:1])
    return df

df1_dataset_details = getDatasetDetails(data_dir)


# REMOVE DUPLICATE SAMPLES

def count_dupe_images(df_):
    d_c = df_.groupby('image').count()
    discard = d_c[d_c.latex>1].index.values
    print(len(discard))
    return len(discard)

def discard_dupe_images(df_):
    d_c = df_.groupby('image').count()
    discard = d_c[d_c.latex>1].index.values
    return df_[~df_.image.isin(discard)]

def unintersect_images(df1_, df2_):
    o = (set(df1_.image) & set(df2_.image))
    print('%d images are intersecting'%len(o))
    return df2_[~df2_.image.isin(o)]

def assert_dupes_are_identical(df_):
    d_c = df_.groupby('image').count()
    discard = d_c[d_c.latex>1].index.values
    for image in discard:
        d_ = df_[df_.image == image]
        latex = d_.latex.iloc[0]
        for i in range(len(d_)):
            assert d_.image.iloc[i] == image
            # assert d_.latex.iloc[i] == latex, '%s \n!=\n %s'%(l, latex)
    
print("-- removing duplicates process")
df2_dataset_details = df1_dataset_details
if df2_dataset_details.shape[0] != df2_dataset_details.image.unique().shape[0]:
    final_shape = (df2_dataset_details.image.unique().shape[0],) + df2_dataset_details.shape[1:]
    print('Removing duplicates will reduce num_samples from %d to %d ... '%(df2_dataset_details.shape[0], df2_dataset_details.image.unique().shape[0]))
    df_cnt = df2_dataset_details.groupby('image').count()
    dupes = df_cnt[df_cnt.latex>1].index.values.tolist()
    for image in dupes:
        ids = df2_dataset_details[df2_dataset_details.image == image].index.values.tolist()
        assert len(ids) > 1
        df2_dataset_details = df2_dataset_details.drop(ids[1:])
    assert df2_dataset_details.shape == final_shape
    print('Final shape = %s'%(final_shape,))
else:
    print('No duplicates found')
print(f"df2_dataset_details shape --> {df2_dataset_details.shape}")

# CLEAN THE FORMULA TEXT
def load_df_clean(data_dir_, df_image_details_):
    NOT_PRINTABLE_CHARS_RE = r'[^\\' + string.printable + r']'
    DELETE_RE = re.compile(r".\x7F")
    PERCENTS_RE = r'%'
    return pd.read_pickle(os.path.join(data_dir_, output_dir, 'df3_clean.pkl'))
    
def make_df_clean(data_dir_, df_image_details_):
    NOT_PRINTABLE_CHARS_RE = r'[^\\' + string.printable + r']'
    DELETE_RE = re.compile(r".\x7F")
    PERCENTS_RE = r'%'

    df = df_image_details_
    # Ensure everything's ascii. str.decode will throw an exception if any non-ascii character is found.
    # If an exception does get thrown, then you'll have to write code to filter out the non-ascii rows.
    # Possibly the code below that filters out all but non-printable ascii chars will suffice, but has
    # not been tested for this purpose.
    
    for index,row in df.iterrows():
        formula = row['latex']
        if(formula.isascii()):
            # print(formula)
            continue
        else:
            print("false")
    
    cleaned = df['latex'].astype(str)
    # Coalesce whitespace to a single space
    cleaned = cleaned.str.replace(r"\s+", ' ')
    # Strip whitespace from the sides
    cleaned = cleaned.str.strip()
    # Discard strings with non-printable characters
    bad1 = df.latex.str.contains(NOT_PRINTABLE_CHARS_RE)
    print ('nonprintables #: ', bad1.sum())
    # Discard strings with embedded percent signs
    bad2 = df.latex.str.contains(PERCENTS_RE)
    print ('percents #: ', bad1.sum())
    good = ~(bad1 | bad2)
    df = df.assign(latex_ascii=cleaned, latex_ascii_len=cleaned.str.len())
    print ('good #: ', good.sum())
    df = df[good]
    return df

print("cleaning dataset -- process")
df3_clean = make_df_clean(data_dir, df2_dataset_details)
print(f"df3_clean shape --> {df3_clean.shape}")
# print(df3_clean.columns)
# print(df3_clean.head)
# print(df3_clean.iloc[:2])
# print(df2_dataset_details.head)
# print(df3_clean.head)
# TODO - Make the below two assert statements work later
# assert df3_clean[df2_dataset_details.formula_len != df3_clean.latex_ascii_len].shape[0] == 0
# assert df3_clean[df2_dataset_details.latex != df3_clean.latex_ascii].shape[0] == 0
df_clean = df3_clean

# CREATE THE TOKEN DICTIONARY
class TokenDict(object):
    def __init__(self):
        self._tokens = {}
    
    def account(self, token_list):
        for token in token_list:
            self._count(token)
            
    def _count(self, token):
        if token in self._tokens:
            self._tokens[token] += 1
        else:
            self._tokens[token] = 1
        return 1
    
    @property
    def dict(self):
        return self._tokens
    
    @property
    def tokens(self):
        return sorted(self._tokens.keys())

def append_special_words(df_vocab_, freq_):
    assert 0 not in df_vocab_.id.values
    df_vocab_ = df_vocab_.append(pd.DataFrame({'id':0, 'freq': freq_}, index=[r'\eos']), verify_integrity=True)
    assert 1 not in df_vocab_.id.values
    df_vocab_ = df_vocab_.append(pd.DataFrame({'id':1, 'freq': freq_}, index=[r'\bos']), verify_integrity=True)
    return df_vocab_

def remove_special_words(df_vocab_):
    return df_vocab_.drop(labels=[r'\eos', r'\bos'])
    
def make_vocabulary(df_, data_dir_, already_tokenized=False):
    ## Split latex into tokens.
    if not already_tokenized:
        ## Isolate latex commands first - i.e.
        ## (optionally even number of backslashes) followed by one backslash followed by letters.
        ## Everything else is a one-character token in itself.
        LATEX_RE = re.compile(r"(?:(?<=\\\\\\\\\\\\)\\[a-zA-Z]+)|(?:(?<=\\\\\\\\)\\[a-zA-Z]+)|(?:(?<=\\\\)\\[a-zA-Z]+)|(?:(?<!\\)\\[a-zA-Z]+)|.")
        sr_token = df_.latex_ascii.str.findall(LATEX_RE)
    else:
        ## Assume that the latex formula strings are already tokenized into string-tokens separated by whitespace
        ## Hence we just need to split the string by whitespace.
        sr_token = df_.latex.str.split(' ')
        
    sr_tokenized_len = sr_token.str.len()
    df_tokenized = df_.assign(latex_tokenized=sr_token, tokenized_len=sr_tokenized_len)
    ## Aggregate the tokens
    vocab = TokenDict()
    sr_token.agg(lambda l: vocab.account(l))
    ## Sort and save
    tokens = []
    count = []
    for t in vocab.tokens:
        tokens.append(t)
        count.append(vocab.dict[t])
    ## Assign token-ids. Start with 2. RESERVE 0 as a 'NULL' token, 1 as BEGIN-SEQUENCE token
    df_vocab = pd.DataFrame({'id':range(2,len(tokens)+2), 'freq':count}, index=tokens, columns=['id', 'freq'])
    df_vocab = append_special_words(df_vocab, df_.shape[0])
    print ('Vocab Size = ', df_vocab.shape[0])
    max_id = df_vocab.id.max()
    print ('Max TokenID = ', max_id, type(max_id))

    if not already_tokenized:
    ## Now ensure that space is the last ID.
    ## This is required by the CTC decoder if we wanted to use space as blank-token for CTC
        max_idx = df_vocab[df_vocab.id == max_id].index[0]
        #print 'max_idx=', max_idx, type(max_idx)
        space_id = df_vocab.loc[' '].id
        #print 'space_id=', space_id, type(space_id)
        df_vocab.loc[' '].id = max_id
        df_vocab.loc[max_idx].id = space_id
        print ('swapped ids %d and %d'%(max_id, space_id))
        print('SpaceTokenID = ', df_vocab.loc[' '])
        
    print(df_tokenized.iloc[:1])
    if dump:
        df_vocab.to_pickle(os.path.join(data_dir_,'df_vocab.pkl'))
        df_tokenized.to_pickle(os.path.join(data_dir_,'df_tokenized.pkl'))
    return df_vocab, df_tokenized

def make_vocabulary2(df_dataset_details_, data_dir_):
    """
    This function shortucts the make_df_clean steps. It assumes that the original latex formulas have
    already been normalized (for e.g. using katex) and tokenized with each token separated by one space
    character. Given this assumption the cleaning step above is not needed. This is roughly how the
    harvardnlp im2latex solution preprocessor creates their vocabulary and I've included this procedure
    here in order to compare my vocabulary with theirs.
    """
    df_dataset_details_ = df_dataset_details_.assign(latex_ascii=df_dataset_details_.latex)
    return make_vocabulary(df_dataset_details_, data_dir_, already_tokenized=True)

def load_vocabulary(df_, data_dir_):
    df_vocab = pd.read_pickle(os.path.join(data_dir_,'df_vocab.pkl'))
    df_tokenized = pd.read_pickle(os.path.join(data_dir_,'df_tokenized.pkl'))        
    return df_vocab, df_tokenized


df_vocab, df_tokenized = make_vocabulary2(df2_dataset_details, data_dir)
df_vocab2, df_tokenized2 = make_vocabulary(df_clean, data_dir)
df_vocab.sort_values(by='id')
set1 = set(df_vocab.index.tolist())
set2 = set(df_vocab2.index.tolist())
pd.Series(list(set2-set1))
df_vocab.loc[(list(set1-set2))].sort_values(by='freq')

## Erroneous tokens in the normalized latex code
df_vocab[df_vocab.index.str.contains('bject')]


# Remove low-frequencey words. Also, remove the words '[object' and 'Object]' which are probably an artifact of
# a bug in harvardnlp latex normalization code because they are not valid latex commands. Also a few others
# that do not produce an output.
def prune_vocab(df_data_, df_vocab_, remove_words, min_freq=24):
    # TODO df_vocab_keep = df_vocab_.drop(remove_words)
    df_vocab_keep = df_vocab_
    df_vocab_keep = df_vocab_keep[df_vocab_keep.freq >= min_freq]
    remove_words = (set(df_vocab.index.values.tolist()) - set(df_vocab_keep.index.values.tolist()))
    print ('Removing the following %d words from the vocabulary: %s'%(len(remove_words), remove_words))
    sr_keep = df_data_.latex_tokenized.map(lambda a: len(remove_words & set(a))==0)
    df_pruned = df_data_[sr_keep]
    kept = df_pruned.shape[0]
    removed = df_data_.shape[0] - kept
    print ('%d samples (%.1f%%) removed'%(removed, removed*100./df_data_.shape[0]))
    print ('df_pruned.shape = %s'%(df_pruned.shape,))
    print(df_pruned[:1])
    
    ## Prune vocabulary
    df_vocab_keep = remove_special_words(df_vocab_keep)
    num_words_keep = df_vocab_keep.shape[0]
    df_vocab_keep = df_vocab_keep.assign(id=range(2, num_words_keep+2))
    df_vocab_keep = append_special_words(df_vocab_keep, df_data_.shape[0])
    # df_vocab_unk = df_vocab_[df_vocab_.index.isin(remove_words)]
#     num_words_unk = df_vocab_unk.shape[0]
#     df_vocab_unk = df_vocab_unk.assign(id=([UnkID]*num_words_unk))
    print ('Vocabulary size reduced from %d to %d'%(df_vocab_.shape[0], df_vocab_keep.shape[0]))
    print ('Pruned vocab shape = %s'%(df_vocab_keep.shape,))    
    return df_pruned, df_vocab_keep

df_data_pruned, df_vocab_pruned = prune_vocab(df_tokenized, 
                                            df_vocab, 
                                            remove_words = set(['[object', 'Object]']), 
                                            min_freq= 24)

df_vocab_pruned.sort_values(by='freq', ascending=False)  # (338, 2)

dict_vocab = df_vocab_pruned.to_dict()

def reverse_dict(d):
    r = {}
    for k in d.keys():
        v = d[k]
        r[v] = k
    return r
dict_id2word = reverse_dict(dict_vocab['id'])
print(dict_vocab['id'])
print(dict_id2word)

print("reverse_dict done successfully")

if dump:
    with open(os.path.join(data_dir, 'dict_vocab.pkl'), 'wb') as f:
        pickle.dump(dict_vocab, f, pickle.HIGHEST_PROTOCOL)
    with open(os.path.join(data_dir,'dict_id2word.pkl'), 'wb') as f:
        pickle.dump(dict_id2word, f, pickle.HIGHEST_PROTOCOL)

print (df_data_pruned.latex_ascii[df_clean.latex.str.contains(r'\\\\\\\\\\\\\\\\\\\\')].count())

def make_word2id(df_tokenized_, df_vocab_):
    word2id = df_vocab_.id.to_dict()
    sr_word2id = df_tokenized_.latex_tokenized.apply(lambda l: map(lambda t: word2id[t], l))
    df_ = df_tokenized_.assign(word2id=sr_word2id, word2id_len=sr_word2id.str.len())
    print(f"df_.word2id_len ==> {df_.word2id_len}")
    print(f"df_.tokenized_len ==> {df_.tokenized_len}")
    assert df_.word2id_len.equals(df_.tokenized_len)
    df_ = df_.drop(labels=['tokenized_len'], axis=1)
    print (df_.shape)
    print(df_[:1])
    return df_

df_word2id = make_word2id(df_data_pruned, df_vocab_pruned)  # 152840

if dump:
    df_word2id.to_pickle(os.path.join(data_dir, 'df_word2id.pkl'))

